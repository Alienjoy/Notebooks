# 1、[GoogLeNet Incepetion V1](https://zhuanlan.zhihu.com/p/73857137) 

### 1.为什么要提出Inception

一般来说，提升网络性能最直接的办法就是增加网络深度和宽度，但一味地增加，会带来诸多问题：
1）参数太多，如果训练数据集有限，很容易产生**过拟合**；
2）网络越大、参数越多，**计算复杂度越大**，难以应用；
3）网络越深，容易出现梯度弥散问题（梯度越往后穿越容易消失），难以优化模型

文章认为解决上述两个缺点的根本方法是将全连接甚至一般的卷积都转化为稀疏连接。一方面现实生物神经系统的连接也是稀疏的，另一方面有文献[1](https://blog.csdn.net/shuzfan/article/details/50738394#fn:1x)表明：对于大规模稀疏的神经网络，可以通过分析激活值的统计特性和对高度相关的输出进行聚类来逐层构建出一个最优网络。**这点表明臃肿的稀疏网络可能被不失性能地简化**

### 2.什么是Inception

Inception就是把多个卷积或池化操作，放在一起组装成一个网络模块，设计神经网络时以模块为单位去组装整个网络结构。模块如下图所示：

![](https://pic3.zhimg.com/80/v2-effa75269cba3c8038c49185e9e8368a_1440w.jpg)

在未使用这种方式的网络里，我们一层往往只使用一种操作，比如卷积或者池化，而且卷积操作的卷积核尺寸也是固定大小的。但是，在实际情况下，在不同尺度的图片里，需要不同大小的卷积核，这样才能使性能最好，或者或，对于同一张图片，不同尺寸的卷积核的表现效果是不一样的，因为他们的感受野不同。所以，我们希望让网络自己去选择，Inception便能够满足这样的需求，一个Inception模块中并列提供多种卷积核的操作，网络在训练的过程中通过调节参数自己去选择使用，同时，由于网络中都需要池化操作，所以此处也把池化层并列加入网络中

> **但是，使用5x5的卷积核仍然会带来巨大的计算量。** 为此，文章借鉴NIN[2](https://blog.csdn.net/shuzfan/article/details/50738394#fn:2x)，采用1x1卷积核来进行**降维**。 
> 例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256，大约减少了4倍。

改进后的Inception

<img src="https://img-blog.csdn.net/20160225155351172" style="zoom:50%;" />

### 3.GoogLeNet的整体结构

<img src="https://img-blog.csdn.net/20160225155414702"  />

**1 .** 显然GoogLeNet采用了模块化的结构，方便增添和修改； 
**2 .** 网络最后采用了average pooling来代替全连接层，想法来自NIN,事实证明可以将TOP1 accuracy提高0.6%。但是，实际在最后还是加了一个全连接层，主要是为了方便以后大家finetune； 
**3 .** 虽然移除了全连接，但是网络中依然使用了Dropout ; 
**4 .** 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。文章中说这两个辅助的分类器的loss应该加一个衰减系数，但看caffe中的model也没有加任何衰减。此外，实际测试的时候，这两个额外的softmax会被去掉。

本文的主要想法其实是想通过构建密集的块结构来近似最优的稀疏结构，从而达到提高性能而又不大量增加计算量的目的。GoogleNet的caffemodel大小约50M，但性能却很优异。

# 2、[GoogLeNet Incepetion V2](https://blog.csdn.net/shuzfan/article/details/50738394)