# 选题的科学意义和应用前景

细粒度图像识别 (fine-grained image recognition，FGIR)，又被称作子类别图像分类(Sub-Category Recognition)，细粒度图像识别是对一个大类别中的子类进行分类识别，如识别出各种鸟的种类，花的品种，车的款式等，在工业界和实际生活中有着广泛的业务需求和应用场景。

细粒度图像识别与一般的跨物种的图像识别相比，需要神经网络学习到更深层次的语义信息，可以应用到工业领域，金融领域，医疗领域，零售领域等等，在很大程度上可以减少工作量，降低成本，提高工作效率。同时智能化正在成为各行各业的发展方向，尤其随着各领域数据量的不断增加，细粒度图像识别技术的研究可以推动机器学习技术的发展，各企业都希望通过数据分析的手段，得到数据中有价值的信息，改善产品、服务和运营。

细粒度图像识别技术有巨大的研究前景和应用价值。例如, 在生态保护中, 有效识别不同种类的生物，是进行生态研究的重要前提。近年来，虽然生态环境取得了明显改善,但是生态环境保护面临的形势依然严峻复杂，生态环境引发的问题也需要我们重视，对于濒危生物的保护更加迫切。利用监控设备通过细粒度图像识别可技术以准确的识别、定位濒危物种，继而对濒危物种展开定点跟踪保护，替代人工实现自动监控，极大减少了工作量。在刑侦领域，细粒度图像识别应用到车辆上可以给案件的侦破提供很大的帮助，案件侦破过程中如果警察初步确定了嫌疑人乘坐车辆型号，则可以利用案发周围的交通摄像头通过细粒度图像识别方法，自动定位嫌疑车辆，给案件的侦破节省宝贵的时间。[4]。

另外随着在线购物平台的发展，以图搜物的功能逐渐成为新的需求，当我们在互联网上看到一款想要商品的图片的时候，就可以直接使用以图搜物的方式在购物网站搜索，但是目前以图搜物功能仍旧准确率较低，由于拍摄角度、设备和光线的不确定性，不能准确搜索到想要的同款商品。细粒度图像识别就可以很好的解决这类问题，提高搜索的准确性。

# 论文主要研究问题

本学位论文研究了细粒度图像识别的模型构建和训练方法，主要研究的内容如下：

1.分类优先级研究

细粒度图像识别目前主要采用的是基于定位-识别的方法，此方法是依据人类在对物体识别时的步骤来实现的。人类在区分相似物体时，首先会找到有区别的区域，然后再对该区域进行比对识别。基于定位识别的方法可以分为强监督细粒度图像识别和弱监督细粒度图像识别。强监督细粒度图像识别是指在训练模型的时候不仅需要图像的类别标签，同时还需要一些额外的人工标注信息，如物体标注框（Object Bounding Box）和部位标注点（Part Annotation）,但是这些标注信息的获取需要大量的人力物力，在实际应用中存在很大的局限性。近年来主流的研究方向倾向于弱监督细粒度图像识别，即只使用图像级的类别标签来实现端到端的图像识别模型，让模型自动提取局部特征并进行分类识别。本论文主要研究弱监督条件下的细粒度图像识别，在基于TransFG模型下进一步探索端到端的细粒度图像识别模型。初步研究思路是对样本进行有针对性训练，即首先重构所有样本的标签，将训练样本分为感兴趣类别和其他类别送入模型进行训练，找出感兴趣类别和其他类别的区别。下一步再在其他类别中选取一类进行标签重构，将重构标签后的数据集重新送到模型中训练。另一方面将训练模型应用遗忘曲线，在一次次训练过程中，模型可以减少模糊、不明确特征的干扰，强化感兴趣类别的特征。

在实际应用中，不需要模型对所有的种类都有很高的预测精准度，只需要保证模型对我们感兴趣类别有较好的预测精准度即可，如濒危物种的识别模型，只需要训练模型对我们需要监控的物种有很高的识别准确率即可，在医学影像领域要保证模型对重大疾病有较高的识别率，严重程度较低的病情可以适当降低预测准确率的标准。对于预测准确率较低的类别，一方面可以通过多次检查来弥补模型的缺陷，另一方面可以对各种病情都单独训练一个模型来进行有针对性的筛查。可以对感兴趣类别进行针对训练

目前主流的研究方向是提高模型对所有样本的预测精确率，在实际应用中不需要保证模型面面俱到，只需要将有限的神经元训练的对感兴趣类别敏感即可，本论文的灵感来源于人类在学习过程中都是将有限的精力和时间用到重要的事情上，对事情都设置一个优先级，在能力范围内优先考虑重要的事情。有侧重点好钢用在刀刃上术业有专攻。同样的细粒度图像识别在实际应用中不需要训练出一个全面型模型，只需要在某一方面具有较强的预测、识别能力即可。

2 特征提取

# 文献综述

**引言**

图像识别技术目前已经广泛应用到我们的日常生活中，从人脸识别门禁系统、刷脸支付和车牌识别系统等等都极大的方便了我们的日常生活。细粒度图像识别作为图像识别技术的一个分支，不同于一般的语义级图像分类和实例级图像分类，它是在识别出基本类别的基础上，进行更精细的子类划分。如识别出花的种类、狗的品种、车的型号等等。由于细粒度图像之间有较小的类间差异和较大的类内差异，不同子类之间属于同一大类，因此他们具有相同的形态、纹理和颜色等，但是同一子类之间由于拍摄图片时被拍摄对象具有不同的姿态，背景和角度，所以在进行细粒度图像识别的过程中往往需要神经网络具有强大的学习能力。近年来, 随着深度学习的发展，深度卷积神经网络为细粒度图像分类带来了新的机遇。大量基于深度卷积特征算法的提出，使得细粒度图像识别得到了进一步的发展。但是细粒度图像识别会造成模型的网络结构体积庞大、参数量过多，使得训练困难。同时模型都是针对所有种类进行的训练，对所有子类不加区别，不能保证对单独每一个类别有较高的识别率，属于全面型模型。但在实际应用过程中只需要保证模型对我们感兴趣的类别有较高的识别准确率即可，本论文主要讨论的是针对特定类别的细粒度图像识别。

**研究背景和意义**

近年来，人工智能发展的越来越快，深刻影响着各行各业，智能化成为各个行业的发展方向。神经网络和深度学习的最新进展极大地推动了计算机视觉技术的发展。计算机视觉领域的蓬勃发展，改变了我们的生产生活方式，给我们的生活带来了极大的便利。同时移动互联网、智能手机以及社交网络的发展带来了海量图片信息，图像分类作为计算机视觉技术最广泛的应用，它是通过计算机来模拟人的视觉感知系统和神经系统，通过自动对图像上的内容进行分析和推理，提取出图像的高级语义信息并判断图像上的内容和所要表达的含义。细粒度图像识别是对一个大类别中的子类进行分类识别，如识别出各种鸟的种类，花的品种，车的款式等，在工业界和实际生活中有着广泛的业务需求和应用场景。细粒度图像识别极具研究意义，并且诸多研究成果已经应用到我们的实际生活中了。

例如：在生态保护中, 有效识别不同种类的生物，是进行生态研究的重要前提。近年来，虽然生态环境取得了明显改善，但是生态环境保护面临的形势依然严峻复杂，生态环境引发的问题也需要我们重视，对于濒危生物的保护更加迫切。利用监控设备通过细粒度图像识别技术可以准确的识别、定位濒危物种，继而对濒危物种展开定点跟踪保护，替代人工实现自动监控，极大减少了工作量。

在刑侦领域，细粒度图像识别应用到车辆上可以给案件的侦破提供很大的帮助。案件侦破过程中如果警察初步确定了嫌疑人乘坐车辆型号，则可以利用城市的交通摄像头通过细粒度图像识别方法，利用车辆的一些辅助信息自动定位嫌疑车辆，提高车辆识别的准确性[4]。

在医疗领域：传统医疗影像由医生进行读片，诊断速度较为缓慢，且完全依赖医生的个人能力，对相关领域专业人才需求量较大。通过细粒度图像识别可以对医学影像进行特征提取、分析和判断，完成病灶的自动识别与标注，降低诊断结果的假阴性概率，具有较高的学术价值和广泛的应用前景。

在生活领域：人们可以通过手机的摄像头来对身边的物品进行识别，了解物品的名称和是否具有危险性，例如可以扫描蔬菜和水果获取商品的微量元素和卡路里等等信息。同时还可以应用到在线购物平台，实现拍照购物，拍照查价格等等。

**发展历程和国内外研究现状**

**发展历程**

​	细粒度图像由于具有较小的类间差异和较大的类内差异，导致对细粒度图像的识别成为一项很有挑战性的任务，特别是针对长尾类的识别更是难上加难。比如鸟类嘴部的形状或腹部毛色等，同时这些细节往往只占据图片的一小部分，这就导致了模型很难准确的找到各物种之间的区别，要求模型有极强的表征学习能力。细粒度图像分类研究，从提出到现在已经经历了一段较长时间的发展，早期采用基于特征提取的传统算法，由于特征的表述能力有限，分类效果也往往面临很大的局限性。随着深度学习的兴起，神经网络能够提取比传统算法有更强大的特征描述能力，在一定程度上极大地促进了细粒度图像分类算法的发展。

**国内外研究现状**

目前细粒度图像识别主要采用基于深度学习的算法，根据监督方式的不同该类方法可以分为强监督细粒度图像识别和弱监督细粒度图像识别。

（1）强监督细粒度图像识别

​	强监督细粒度图像识别是指在模型训练时为了获得更好的分类精度，除了使用图像的类别标签外还使用物体标注框（Object Bounding Box）和部位标注点（Part Annotation）等额外的人工标注信息，标示出目标物的位置和目标物各部位的位置。例如， Zhang等人提出了专门针对细粒度图像识别的Part-based R-CNN算法，其总体流程图如下所示。首先利用Selective Search等算法在细粒度图像中产生物体或物体部位可能出现的候选框（object proposal），之后类似于R-CNN做物体检测的流程，借助细粒度图像中的object bounding box和part annotation可以训练出三个检测模型（detection model），然后对三个检测模型得到的检测框加上位置几何约束，这样便可得到较理想的物体／部位检测结果。接下来将得到的图像块（Image Patch）作为输入，分别训练一个CNN，则该CNN可以学习到针对该物体／部位的特征。最终将三者的全连接层特征级联（Concatenate）作为整张细粒度图像的特征表示。显然，这样的特征表示既包含全部特征（即物体级别特征），又包含具有更强判别性的局部特征（即部位特征：头部特征/躯干特征）。

![img](/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70.png)

​		沿着Part-based RCNN方法的思路，S. Branson等人提出了Pose Normalized CNN的方法。Pose Normalized CNN使用DPM算法获取物体级别和部位级别的检测框，同时对部位级别图像块做了姿态对齐操作。由于CNN不同层的特征具有不同的表示特性（如浅层特征表示边缘等信息，深层特征更具高层语义），该算法提出应针对细粒度图像不同级别的图像块，提取不同层的卷积特征。然后将它们级联起来作为图像的最终特征，从而可以融合多层语义信息。如此的姿态对齐操作和不同层特征融合方式，使得Pose Normalized CNN在使用同样多标记信息时取得了相比Part-based R-CNN高2%的分类精度。

​		进一步发展，Wei[2]等人提出了一种新颖了端到端的模型Mask-CNN，是一种新型的深度区域检测和描述符选择方案，其网络架构如图2所示。该模型不需要全连接层进行细粒度图像识别，因此有较小的参数量和特征纬度。该模型通过使用部位标注点来将对象分为头部和躯干两个点集，然后取这些点的最小外接矩形作为ground-truth mask，其他的点为背景，然后利用全卷积网络（FCN）生成用于局部定位和选择有用的深度描述符的掩膜版。基于这些对象/部位蒙版，构建了4流的Mask-CNN，实现了同时进行对象级和部件级描述符联合训练的方法，在CUB-200-2011数据集达到85.5%的精度。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211031114259772.png" alt="image-20211031114259772" style="zoom:50%;" />

​		强监督细粒度识别算法虽然取得了较好的识别准确率，但都需要依靠于附加的人工标注信息（如对象标注框、部位标注点）来提供识别对象的位置和大小信息，往往这些标注信息的获取需要大量的人力资源和成本，这在一定程度上限制了此类算法在实际生活场景中的应用。弱监督细粒度图像识别是仅利用图像的类别标注信息即可完成细粒度图像的分类任务，大大提高了模型的泛化能力，降低了获取训练样本的成本，因此此类方法获得了大量的关注和研究，是目前细粒度图像识别的主流方法。弱监督细粒度图像识别算法根据实现的方式不同又可以分为以下两种：基于局部区域的图像特征学习[10]和基于双线性池化的图像特征学习[16,48]。

​		两级注意力(Two Level Attention)算法第一个尝试不依赖额外的标注信息，而仅仅使用类别标签来完成细粒度图像分类的工作，是基于局部区域的图像特征学习。该方法使用深度卷积网络对局部区域进行定位，然后对每个区域进行细粒度特征提取。最后将对象级和部位级的细粒度特征级连起来作为整张细粒度图像的特征表示，来进行细粒度图像的分类。该方法的架构图如图三所示，两级注意力机制结合了三种注意力机制：生成候选图像块的自底向上注意力、选择相关块形成特定物体的对象级自顶向下注意力、定位判别性部件的部件级自底向上注意力。通过对象级过滤器将生成候选图像块进行分类，过滤掉无关的背景。然后再通过部位级检测器得到物体级别的分类结果，然后将对象级和部位级的特征级连起来送入SVM分类器进行训练，该算法在CUB-200数据集上取得了75.7%的精度

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20211031123722945.png" alt="img" style="zoom:50%;" />

​		与基于局部区域的方法不同，双线性池化（Bilinear Pooling）的图像特征学习通过获取图像的高阶特征编码，直接学习更具判别性的全局特征表示，使神经网络有了更强大的学习能力，并且不需要复杂的局部区域定位模块。双线性池化系统架构如图四所示，它是用两个对立的CNN模型对图像提取特征信息，然后对两个流的特征向量进行外积运算和L2归一化进行特征融合，最后通过分类器完成分类任务。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3h5czQzMDM4MV8x,size_16,color_FFFFFF,t_70-20211031143128784.png" alt="img" style="zoom:50%;" />

# 研究内容概述

细粒度图像识别旨在从子类类别中识别出对象，由于要找出各子类之间细微的差别，本身就是一项富有挑战性的任务。最近的研究主要集中如何定位出最具辨别力的图像区域，并以此提高网络捕捉子类细微差别的能力。可是这种策略不可避免的会使得网络结构变得复杂，同时会使得定位到的区域包含对象的大部分位置，导致系统训练困难。最近Vision Transformer(ViT)在传统图像领域表现出强大的性能。ViT由于其固有的Self-attention机制，不采用RNN顺序结构使得模型可以并行训练，ViT应用到目标检测和语义分析证明了其具有强大的捕捉局部信息和全局信息的能力。

### 自注意力机制

自注意力（Self-attention）机制实现了和RNN类似的功能，同样都是输入一个序列输出一个序列，但是ViT的每一个输出都看过全部的输入序列。假如RNN的输入序列为a1，a2，a3，a4，输出序列为b1，b2，b3，b4，RNN计算b4的时候需要看过a1、a2、a3、a4 ，因为RNN的结构决定了它并不能够并行化训练。要实现并行化训练我们可以通过CNN的理论来实现，下图中黄色三角是一个卷积核，每次对三个输入进行卷积，然后进行滑动到序列尾部，输出为橙色圆点，使用另外一个卷积核进行滑动卷积输出为黄色圆点，两个卷积核的计算的没有先后顺序，完全可以同步进行，这就使得ViT可以实现并行化训练。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/v2-cabda788832922a8f141542a334ccb61_r.jpg" alt="preview" style="zoom:50%;" />

根据上面的分析，我们要设计一个自注意力层，它的输入和输出和RNN一样，都是输入一个序列，输出也是一个序列。它的每一个输出b1、b2、b3、b4都看过整个输入序列，但是跟RNN不同的是它可以实现并行化计算。自注意力层的结构如下图所示，输入x1-x4是一个序列，每一个输入是一个向量，每一个x乘以一个矩阵W得到嵌入向量a，每一个a向量再分别乘以变换矩阵Wq、Wk、Wv得到三个向量q、k、v，然后q和k向量作内积得到alpha，然后对每一个alpha做softmax得到alpha hat，然后对v做加权和得到b。

输入序列为I输出序列为O，自注意力层的运算过程为公式1：
$$
Q=W^{q}I\\
K=W^{k}I\\
V=W^{v}I\\
A=K^{T}Q\\
O=softmax(A)V
$$
所以自注意力机制是一堆矩阵的乘法，可以用GPU实现加速。多头注意力机制是a乘以不同的Wq，Wk，Wv，多头注意力是由多个自注意力机制组成的。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/v2-b7e1ffade85d4dbe3350f23e6854c272_r.jpg" style="zoom:50%;" />



### TransFG架构

1. 算法简介

TransFG是基于transformer框架的针对细粒度图像识别的分类算法，TransFG将transformer的所有原始注意力权重整合到一个attention map中，用于指导网络高效而准确地选择具有辨别力的图像块并计算它们之间的的关系。

TransFG算法首先将输入图像进行切分成多个小的图像块，组成一个输入矩阵。TransFG模型生成输入矩阵的方法不同于ViF，它是将图片进行重叠切分，使得各图像块之间图像有重合。然后对输入矩阵进行线性投影，并加上表示每个图像块位置信息的位置编码。接着将经过embedding的输入矩阵经过多个Transformer层进行转换，最后利用部位选择模块选择哪一个图像块作为最后一个Transformer层的输入。由于最后一层Transformer只输入了目标区域对应的标记，迫使最后一个Transformer层关注于不同子类之间的细微差别，并忽视诸如背景等差异较小的区域。最后将变换后的矩阵送入分类器进行分类，并选用交叉熵损失函数和对比损失函数来同时来训练网络参数。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211102210215787-5858137.png" alt="image-20211102210215787" style="zoom:50%;" />



1. vision transformer的输入把图像切分成patch，但是是没有overlap的，文章改成切分patch用overlap（这只能算个trick）
2. part selection module

Part Selection Module（PSM）的作用是选择出权重较大的图像块拼接起来作为最后一个Transformer层的输入。PSM是把最后一层前的所有层的权重进行矩阵乘法，然后选择其中较大的注意力索引，以提取ZL-1中对应的标记。最后将选定的分类标记作拼接起来为最后一个Transformer层的输入序列。由于选择了最具有辨别性的区域替换原来的输入序列，抛开了一些背景和辨别性较小的区域，不仅保留了全局信息，同时强迫最后一个Transformer层关注不同子类之间的细微差别。迫使网络学习比较重要的部分信息，提高了网络的分类性能。

假设最后一个Transformer层原本的输入为，其中ZL-1N表示第L-1层的n个输出。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211104095642213-5991003.png" alt="image-20211104095642213" style="zoom:50%;" />

前面某一层的网络参数的权重为，下标l表示层数，每一层Transformer都是由多头注意力组成的，每个head的权重为ali

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211104103758636-5993479.png" alt="image-20211104103758636" style="zoom:50%;" />

其中ali表示第i个head的权重，上标表示head的索引。

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211104104517340-5993919.png" alt="image-20211104104517340" style="zoom:50%;" />

则最后一层前面所有层的权重和为：

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211104104657186-5994018.png" alt="image-20211104104657186" style="zoom:50%;" />

由于afinal捕捉到了信息如何从输入层传播到更高层，与单层原始注意权重a L-1相比，它可以更好的选择判别性区域。选择最大值A1，A2，...，AK表示最后的K个不同注意力头的索引。这些位置被用作模型的索引，以提取z L-1中相应的标记。最后将选定的标记与分类标记连接起来，作为最后一个Transformer层的输入序列，表示为：

<img src="/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211104111517090-5995718.png" alt="image-20211104111517090" style="zoom:50%;" />



1. 损失函数

在细粒度图像识别算法中由于子类别之间的差异较小，一般的交叉熵损失函数不足以监督特征的学习。对比损失函数可以类比于弹簧模型，对于同一类样本，各样本特征点之间的距离是正距离，此时同类样本特征点之间存在吸引力，促使模型往缩特征点距离的方向更新参数。对于不同类样本，各特征点之间的距离是负距离，此时不同样本特征点之间存在斥力，促使模型往扩大特征点距离的方向更新参数。

对比损失函数如下，其中Dw表示样本特征点之间的欧式距离，m表示同一类别之间特征点最大距离的阈值，N表示样本数量。当样本属于同一类时Y=1，此时损失函数只剩下前半部分，即原本相似的样本，如果在特征空间的欧式距离较大，则说明当前的模型不好，因此加大损失函数。当样本不属于同一类别时Y=0，此时损失函数只剩下后半部分，即当样本不相似时，其特征空间的欧式距离反而小的话，损失值会变大。

![image-20211118192110638](/Users/zhangshuheng/Desktop/Notebooks/图像分类/细粒度图像识别/00开题报告.assets/image-20211118192110638.png)

对比损失函数可以缩小在高维空间中同一类别之间的距离，扩大不同类别之间的距离，细粒度图像识别算法采用对比损失函数可以捕捉到不同子类之间的细微差别。



# 预期解决的主要问题

### 问题1:模型参数过大，训练困难的问题

### 问题2:对优先级较高类别提高识别准确率

（提高模型参数对高优先级样本的权重，对感兴趣样本敏感）

### 问题3:在实际应用过程中输入样本质量参差不齐差，干扰较多造成识别率下降的问题，抗干扰能力。



