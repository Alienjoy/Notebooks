# 深度卷积神经网络的图像识别

# 1、本文的贡献和创新点

a、在ILSVRC-2010和ILSVRC-2012[2]的ImageNet子集上训练了到目前为止最大的神经网络之一。

b、我们编写了高度优化的2D卷积GPU实现以及训练卷积神经网络内部的所有其它操作。

c、我们的网络包含许多新的不寻常的特性，这些特性提高了神经网络的性能并**减少了训练时间**，详见第三节。

d、我们使用了一些有效的技术来防止过拟合，详见第四节。

e、深度似乎是非常重要的：我们发现移除任何卷积层（每个卷积层包含的参数不超过模型参数的1%）都会导致更差的性能。

## 2、ImageNet Large-Scale Visual Recognition Challenge (ILSVRC)

`top-5`错误率是指测试图像的正确标签不在模型认为的五个最可能的便签之中。

# 3、架构

包含八个学习层--5个卷积层和3个全连接层

### 3.1ReLU非线性

Rectified Linear Units修正线性单元作为**激活函数**，函数为<img src="https://bkimg.cdn.bcebos.com/pic/f31fbe096b63f624775e13e08b44ebf81b4ca3d5?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5/format,f_auto" alt="img" style="zoom: 25%;" />是非饱和的（因为激活函数会将输出结果缩放到无穷区间），而饱和函数如`Sigmoid`和`tanh(x)`函数，会将输出结果缩放到有限的区间。

ReLu激活函数的优点：采用ReLU的深度卷积神经网络训练时间比等价的`tanh`单元要快几倍

[过拟合](https://blog.csdn.net/u012950413/article/details/80376136)：就是神经网络的识别率随着训练次数的增加不会再提高了，我们之前把数据集切分为训练集，测试集和验证集。验证集就是用来检测模型overfitting。 一旦在验证集上，准确性不在变化(收敛了),那么我们就停止训练

[正则化](https://www.zhihu.com/question/20924039)：是为了防止过拟合，剔除不是很好的数据。减小求出错误解的可能性。

[归一化](https://www.jianshu.com/p/95a8f035c86c)：把数据变成(０，１)或者（1,1）之间的小数。

[泛化](https://blog.csdn.net/sc2079/article/details/103090727)：泛化（generalization）是指训练好的模型在**前所未见的数据**上的性能好坏，总结规律的能力。

###  3.2多GPU训练

多GPU并行运行，但是并不是所有的层都进行GPU的通信，只在特定层进行GPU之间的数据通信。这个方案降分别低了`top-1 1.7%`，`top-5 1.2%`的错误率。双GPU网络比单GPU网络稍微减少了训练时间。

### 3.3局部响应归一化

局部响应归一化有助于泛化
$$
b_{x, y}^{i}=a_{x, y}^{i} /\left(k+\alpha \sum_{j=\max (0, i-n / 2)}^{\min (N-1, i+n / 2)}\left(a_{x, y}^{j}\right)^{2}\right)^{\beta}
$$
响应归一化分别减少了`top-1 1.4%`，`top-5 1.2%`的错误率

### 3.4 重叠池化

CNN中的池化层归纳了同一核映射上相邻组神经元的输出,更确切的说，池化层可看作由池化单元网格组成，网格间距为s(步长)个像素，每个网格归纳池化单元中心位置z×z（窗口大小）大小的邻居。如果设置s=z，我们会得到通常在CNN中采用的传统局部池化。如果设置s<z，我们会得到重叠池化，即中间的一部分首先会被池化一次，然后下一个步长还会再次池化一次。

### 3.5 整体架构

![卷积神经网络](http://noahsnail.com/images/alexnet/Figure_2.jpeg)

网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax（归一化函数）的输入，产生了一个超过1000个标签的分类器

## 4、减少过拟合

用来克服过拟合的两种主要方式。

### 4.1 数据增强

图像数据上最简单常用的用来减少过拟合的方法是使用**标签保留变换**（label-preserving transformations）来人工增大数据集

**第一种数据增强方式：**包括产生图像变换和水平翻转。我们从256×256图像上通过随机提取224 × 224的图像块实现了这种方式，然后在这些提取的图像块上进行训练。这通过一个2048factor增大了我们的训练集，尽管最终的训练样本是高度相关的。没有这个方案，我们的网络会有大量的过拟合，这会迫使我们使用更小的网络。在测试时，网络会提取5个224 × 224的图像块（四个角上的图像块和中心的图像块）和它们的水平翻转（因此总共10个图像块）进行预测，然后对网络在10个图像块上的softmax层进行平均

**第二种数据增强方式：**数据增强的第二种形式包括改变训练图像中RGB通道的强度。具体来说，我们对整个ImageNet训练集的RGB像素值集进行PCA。对每张训练图像，我们将发现的主成分的倍数加入，其大小与相应的特征值乘以从平均数为零、标准差为0.1的高斯中抽取的随机变量成正比。

### 4.2Dropout

[Dropout](https://zhuanlan.zhihu.com/p/38200980)将每个隐藏神经元的输出设置为零，概率为0.5。以这种方式被 "丢弃 "的神经元对前传没有贡献，也不参与反向传播。所以每次输入时，神经网络都会对不同的架构进行采样，但这些架构都共享权重。这种技术减少了神经元复杂的共同适应性，因为一个神经元不能依赖特定的其他神经元的存在。因此，它被迫学习更健壮的特征，

## 5、更多细节

AlexNet使用随机梯度下降算法，batch大小是128，动量衰减参数设置为0.9，权重衰减参数为0.0005，这里的权重衰减不仅仅是一个正规化器，同时它减少了模型的训练误差，权重 的更新过程变为： 其中， 是迭代次数索引， 是momentum变量， 是学习速率， 是第 个batch中 的梯度的平均值。

另外，在AlexNet中，所以层的权重 初始化为服从0均值，标准差为0.001的高斯分布，第2、4、5卷积层以及全连接层的偏置量 初始化为1，这样做的好处是它通过给ReLU函数一个正激励从而加速早期学习的速度。其他层的偏置量初始化为0.

## 6、实验结果

