# 泰迪杯竞赛源文件

```python
import matplotlib.pyplot as plt
import numpy as np  # 注意numpy要使用1.19.5
import os
import tensorflow as tf

from tensorflow.keras.preprocessing import image_dataset_from_directory

import cv2  # opencv是对图像进行预处理的模块
import pandas as pd  # 数据分析的API，用来处理CSV文件，有DataFrame和Series
# from torch.utils.data import DataLoader, Dataset
import time  # 用来获取当前时间，计算每个epoch的时间

'''
readFile(path)函数是读入图片的函数。
参数：
    path为所要读取图片的文件夹
返回值：
    返回numpy.ndarray类型的多维数组
    返回的是读入的图像组成的多维数组
    大小为(imageNum,imageLength,imageWidth,channel)
'''


def readFile(path):
    sizeWH = 160
    fileName_list = os.listdir(path)  # 会输出一个列表，利用列表的sort方法可以对其进行排序
    fileName_list.sort(key=lambda x: int(x.split('-')[0]))  # 关键字参数接收表达式，对列表中的每一项进行计算，然后用于整个排序过程
    print(fileName_list)
    data_x = np.zeros((len(fileName_list), sizeWH, sizeWH, 3), dtype=np.float32)
    for i, fileName in enumerate(fileName_list):
        image = cv2.imread(os.path.join(path, fileName))
        data_x[i, :, :] = cv2.resize(image, (sizeWH, sizeWH))
    return data_x


'''
readLabel(name)函数是读取csv文件的函数，利用的pandas模块，pandas有DataFrame和Series两种数据结构。
参数:
    name是csv文件的文件名
返回值：
    返回的是numpy.ndarray类型的一维数组
    大小为(labelNum,)
'''


def readLabel(name):
    df = pd.read_csv(name, encoding='gbk')  # 英文用encoding = 'big5'
    # df类型为pandas.core.frame.DataFrame，df['样本类别']的类型为Series
    str_num = {'深灰色泥岩': 0, '黑色煤': 1, '灰色细砂岩': 2, '浅灰色细砂岩': 3, '深灰色粉砂质泥岩': 4,
               '灰黑色泥岩': 5, '灰色泥质粉砂岩': 6}
    for name in str_num:
        df = df.replace(name, str_num[name])
    data_y = df['样本类别'].tolist()  # 把pandas.core.series.Series的数据类型转为list
    # train_y = list(map(int, train_y))#将train_y列表中字符串转为数字，返回值为map类型，在用int转为整数型
    data_y = np.asarray(data_y)  # 再将list类型的数据转为ndarray，转为numpy模块的一维数组
    data_y = data_y.astype(np.int32)
    return data_y


'''
主函数
'''
rockpath = './rock'
train_x = readFile(rockpath)
labelName = 'rock_label.csv'
train_y = readLabel(labelName)
train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))

rockpath = './rock1'
test_x = readFile(rockpath)

train_dataset = train_dataset.repeat(10).shuffle(315 * 10).batch(8)  # repeat之后shuffle，会在 epoch 之间打乱数据
train_batches = tf.data.experimental.cardinality(train_dataset)
validation_dataset = train_dataset.take(train_batches // 5)
train_dataset = train_dataset.skip(train_batches // 5)
print(train_dataset)

# 1。数据增强
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),  # 随机翻转
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),  # 随机旋转，角度为0.2*2π
    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2),  # 0.2
    # tf.keras.layers.experimental.preprocessing.RandomContrast(0.5),#0.2
    # tf.keras.layers.experimental.preprocessing.RandomCrop(96,96),#0.2
])
'''
for image, _ in train_dataset.take(10):
  plt.figure(figsize=(10, 10))
  #print(image)#验证了image的形状为shape=(32, 160, 160, 3)，因为take(1)是取一个batch
  first_image = image[0]#取batch的第一个图像数据

  plt.title('对图片进行随机旋转')
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    if i==0:
        augmented_image = tf.expand_dims(first_image, 0)  # 每做一次循环就做一次数据增强
    else:
        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))#每做一次循环就做一次数据增强
    plt.imshow(augmented_image[0]/255)#因为augmented_image[0]就是float的数据类型，所以不用除255.0  imshow函数参数可以是(0-1 float or 0-255 int)
    plt.axis('off')

  plt.show()
'''
# 2。Rescale pixel values到[-1,1]
preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input

# 3。创建基础模型
# Create the base model from the pre-trained model MobileNet V2
IMG_SHAPE = (160, 160, 3)  # IMG_SHAPE=(160,160,3)元组与元组相加
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,  # MobileNetV2方法会返回一个keras.Model实例。
                                               include_top=False,
                                               weights='imagenet')  # weight来自imageNet，也可以用要加载权重文件的路径。

base_model.trainable = False  # 冻结所有的层layer

# 4。 feature extractor layers特征提取层
global_average_layer = tf.keras.layers.GlobalAveragePooling2D()

prediction_layer = tf.keras.layers.Dense(7)

# 构建模型
inputs = tf.keras.Input(shape=(160, 160, 3))  # 输入，参数为表示图片大小(width,height,channel)的元组

x = data_augmentation(inputs)  # 数据增强
x = preprocess_input(x)  # Rescale
x = base_model(x, training=False)  # 基本模型
x = global_average_layer(x)  # 全局平均pooling
x = tf.keras.layers.Dropout(0.2)(x)  # Dropout
outputs = prediction_layer(x)  # 全连接层，分类层

model = tf.keras.Model(inputs, outputs)

# 编译模型
base_learning_rate = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
print(model.summary(line_length=150))
print("Number of layers in the base model: ", len(base_model.layers))

# print(len(model.trainable_variables))打印可以训练的变量的个数，注意变量不是参数，一共有 1281个可以训练的参数，分为两组变量
initial_epochs = 2
history = model.fit(train_dataset,
                    # batch_size=32,
                    validation_data=validation_dataset,
                    epochs=initial_epochs,
                    )  # history是用来画图的。

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()), 1])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.ylabel('Cross Entropy')
plt.ylim([0, 2.0])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

# 微调
base_model.trainable = True
print("Number of layers in the base model: ", len(base_model.layers))

# 从这层开始微调
fine_tune_at = 150

# 在`fine_tune_at` 层之前的层冻结。
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate / 10),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
print(model.summary(line_length=150))
fine_tune_epochs = 2
total_epochs = initial_epochs + fine_tune_epochs

history_fine = model.fit(train_dataset,
                         epochs=total_epochs,
                         initial_epoch=history.epoch[-1]+1,
                         validation_data=validation_dataset
                         )

probability_model = tf.keras.Sequential([model,
                                         tf.keras.layers.Softmax()])  # 要用做prediction model的话需要再加一个softmax layer
predictions = probability_model.predict(test_x)
print(predictions)
'''
将预测数据存入CSV文件中
'''
label = ['深灰色泥岩', '黑色煤', '灰色细砂岩', '浅灰色细砂岩', '深灰色粉砂质泥岩', '灰黑色泥岩', '灰色泥质粉砂岩']
prediction_y = []
for i in range(len(predictions)):
    prediction_y.append(label[np.argmax(predictions[i])])
data = pd.read_csv('第一问结果.csv')
print(data['样本编号'])
data['样本类别'] = prediction_y
data.to_csv("第一问结果.csv", mode='w', index=False)

acc += history_fine.history['accuracy']
val_acc += history_fine.history['val_accuracy']

loss += history_fine.history['loss']
val_loss += history_fine.history['val_loss']
plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.ylim([0, 1])
plt.plot([initial_epochs - 1, initial_epochs - 1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.ylim([0, 2.0])
plt.plot([initial_epochs - 1, initial_epochs - 1],
         plt.ylim(), label='Start Fine Tuning')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()

```

