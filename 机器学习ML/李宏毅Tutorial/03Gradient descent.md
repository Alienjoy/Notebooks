# 1. Adagrad

__è‡ªé€‚åº”æ¢¯åº¦ä¸‹é™æ³•__:Adaptive Gradient descent

ä¾‹å¦‚ï¼š$learning\ rate=\frac{\eta }{\sqrt{1+t}}$ æ„æ€å°±æ˜¯éšç€æ¯ä¸€æ¬¡iteration learning rateè¦ä¸‹é™ï¼Œä¸‹é¢çš„å¼å­æ˜¯å¯¹äºæ¯ä¸€ä¸ªå‚æ•°wè€Œè¨€ï¼Œå³æ˜¯å¯¹æŸä¸€ä¸ªwçš„æ¢¯åº¦gä¹‹å’Œã€‚
$$
å‡æ–¹æ ¹rms=root\ mean\ square=\sqrt{\frac{x_{1}^2+s_{2}^2+s_{3}^2+.....+s_{n}^2}{n}}
$$

$$
w^{t+1}=w^{t}-\frac{\eta}{\sqrt{\sum_{i-0}^{t}}(g^i)^2}g^t
\\
g=gradientæ¢¯åº¦
$$

åŸç†ï¼š

å°±æ˜¯åœ¨æ¢¯åº¦æ¯”è¾ƒå¤§çš„éƒ¨åˆ†ï¼ˆæ¯”è¾ƒé™¡ï¼Œåˆ†æ¯éƒ¨åˆ†å¤§ï¼‰ï¼Œéœ€è¦è®¾ç½®æ­¥ä¼æ¯”è¾ƒå°æ­¥ï¼ˆæƒé‡æ›´æ–°æ…¢ä¸€ç‚¹ï¼‰ï¼Œé˜²æ­¢å‚æ•°æ›´æ–°å¤ªå¤§è·³è¿‡æœ€å°å€¼ç‚¹

åœ¨æ¢¯åº¦æ¯”è¾ƒå°çš„éƒ¨åˆ†ï¼Œæ¯”è¾ƒå¹³ç¼“ï¼Œåˆ™å¯ä»¥è®¾ç½®æƒé‡æ›´æ–°çš„å¿«ä¸€ç‚¹ï¼ŒåŠ å¿«Lossçš„å˜åŒ–ï¼Œå¯ä»¥åŠ å¿«trainingçš„é€Ÿåº¦ã€‚

**æƒ³æ³•**ğŸ’¡ï¼š

å¯ä»¥åªè®¡ç®—ç¦»è¿™ä¸ªç‚¹æ¯”è¾ƒè¿‘çš„æ¢¯åº¦çš„å¹³æ–¹ï¼Œä¸éœ€è¦è®¡ç®—å…¨éƒ¨çš„æ¢¯åº¦çš„å¹³æ–¹

<img src="https://i.loli.net/2021/03/15/oXZ2pmRJuE4vPd1.png" alt="image.png" style="zoom: 25%;" />



# 2. SGD

**SGDéšæœºæ¢¯åº¦ä¸‹é™æ³•**Stochastic Gradient descent

åŸç†ï¼š

å°±æ˜¯ç”¨éšæœºä¸€ä¸ªsampleæ¥è®¡ç®—Lossï¼Œè€Œä¸æ˜¯ç”¨å…¨éƒ¨çš„sampleè®¡ç®—Loss=sum(y-Y)^2

<img src="https://i.loli.net/2021/03/16/adR1YsMFm5uzSTl.png" style="zoom:25%;" />

ä¼˜ç‚¹ï¼š

å‡è®¾æœ‰20ä¸ªsampleï¼Œå‡å¦‚ç”¨gradient descentï¼Œè¦20ä¸ªsampleçš„Losséƒ½è®¡ç®—å®Œåæ‰æ›´æ–°ä¸€æ¬¡æƒé‡ï¼Œè€ŒSGDåˆ™æ¯ä¸€ä¸ªsampleéƒ½æ›´æ–°ä¸€æ¬¡æƒé‡ï¼Œåˆ™æ›´æ–°20æ¬¡å‚æ•°ï¼Œæ›´æ–°çš„æ›´å¿«

<img src="https://i.loli.net/2021/03/16/ok2rR3sUnlp4ujZ.png" alt="image.png" style="zoom:25%;" />

# 3.feature scaling ã€Standardization



[**æ–¹å·®**](https://zhuanlan.zhihu.com/p/83410946)ï¼ˆå·®çš„å¹³æ–¹ï¼Œvarianceï¼‰ï¼š
$$
D(x)=\sum [x-\overline{x}\ ]^2
$$


**æ ‡å‡†å·®**ï¼ˆstandard deviationï¼‰åˆå«**å‡æ–¹å·®**ï¼š
$$
\sigma =\sqrt{\frac{1}{n}\sum_{i=1}^{n} [x_i-\overline{x}\ ]^2}
$$
ç­çº§å¹³å‡æˆç»©æ˜¯70åˆ†ï¼Œæ ‡å‡†å·®æ˜¯9ï¼Œåˆ™ï¼ˆ61ï¼Œ79ï¼‰åˆ†çš„äººçš„æ¦‚ç‡ä¸º68%,(52,88)åˆ†äººçš„æ¦‚ç‡ä¸º95%

![](https://pic4.zhimg.com/80/v2-5a369710775af7bfd07139ec5a7eb06b_1440w.jpg)

**å‡æ–¹è¯¯å·®**ï¼ˆmean squared errorï¼‰
$$
mse=\sum_{i=1}^{n}\frac{(x_{i}-\overline{x}\ )^2}{n}
$$
**feature scalingï¼ŒStandardization**å°±æ˜¯æŠŠå¤šä¸ªfeatureçš„åˆ†å¸ƒå½’ä¸€åŒ–ï¼Œç¼©æ”¾æˆä¸€æ ·çš„èŒƒå›´ï¼Œæ–¹æ³•
$$
x_i=\frac{x_i-\overline{x}}{\sigma_i}
$$
å°±æ˜¯å¯¹`æ¯ä¸€ä¸ªsample` ç”¨`ç¬¬iä¸ªfeature`å‡å»`ç¬¬iä¸ªfeature`çš„å¹³å‡å€¼å†é™¤ä»¥`ç¬¬iä¸ªfeatureçš„æ ‡å‡†å·®`è¿›è¡Œå¤„ç†

å¯¹æ¯ä¸ªfeatureåšå®Œfeature scalingåï¼Œå¹³å‡å€¼ä¸º0ï¼Œå‡æ–¹è¯¯å·®ä¸º1

### Normalization Standardization

<img src="/Users/zhangshuheng/Desktop/Notebooks/æœºå™¨å­¦ä¹ ML/æå®æ¯…Tutorial/03Gradient descent.assets/image-20220223205121234.png" alt="image-20220223205121234" style="zoom:50%;" />

# 4. æ¢¯åº¦ä¸‹é™æ³•çš„åŸç†

1. åœ¨Losså¹³é¢éšä¾¿æ‰¾ä¸€ä¸ªç‚¹L0ï¼Œç„¶åä»¥è¿™ä¸ªç‚¹ä¸ºåœ†å¿ƒç”»åœ†ï¼Œæ‰¾å‡ºåœ†ä¸ŠLossæœ€å°çš„ç‚¹L1
2. ç„¶åä»¥L1ä¸ºä¸­å¿ƒç”»åœ†ï¼Œæ‰¾å‡ºåœ†ä¸ŠLossæœ€å°çš„ç‚¹ï¼Œå¾ªç¯ä¸€ç›´æ‰¾åˆ°minima

æ‰¾åœ†ä¸ŠLossæœ€å°ç‚¹çš„æ–¹æ³•ï¼šï¼ˆå‡è®¾åœ†ç‚¹ä¸º$(\theta_1=a,\theta_2=b)$æœ‰ä¸¤ä¸ªæƒé‡wåˆ†åˆ«ä¸º$\theta_1,\theta_2$ï¼‰

æ ¹æ®æ³°å‹’å±•å¼€å¼ï¼š
$$
L(\theta)\approx L(a,b)+\frac{\partial L(a,b)}{\partial\theta_1}(\theta_1-a)+\frac{\partial L(a,b)}{\partial\theta_2}(\theta_2-b)
\\
u=\frac{\partial L(a,b)}{\partial\theta_1},\upsilon =\frac{\partial L(a,b)}{\partial\theta_2}
$$


<img src="https://i.loli.net/2021/03/15/xkLcNMgf9U1RDHt.png" alt="æˆªå±2021-03-15 ä¸‹åˆ9.46.09.png" style="zoom:25%;" />

å¦‚æœè¦$L(\theta)$æœ€å°çš„è¯ï¼Œåˆ™$u\Delta \theta _1+\upsilon \Delta \theta _2$æœ€å°ï¼Œ$\overrightarrow{a}=ï¼ˆu,vï¼‰,\overrightarrow{b}=ï¼ˆ\Delta \theta _1,\Delta \theta _2ï¼‰$,åˆ™$u\Delta \theta _1+\upsilon \Delta \theta _2=\overrightarrow{a}\textbf{.}\overrightarrow{b}$æœ€å°ï¼Œå†…ç§¯æœ€å°çš„è¯åˆ™a,bå‘é‡æ–¹å‘ç›¸åï¼Œbå‘é‡é•¿åº¦æœ€é•¿ï¼Œå³$\vec{a}=-\eta \vec{b}$

<img src="https://i.loli.net/2021/03/15/mKafgrRDtuhvWo7.png" alt="image.png" style="zoom:25%;" />

<img src="https://i.loli.net/2021/03/16/ciQps2oqkl7vRDj.png" alt="image.png" style="zoom:25%;" />

# 5. Optimization for Deep Learning

off-lineå°±æ˜¯æ‰€æœ‰sampleæ ·æœ¬çš„xå‘é‡å’Œæ‰€æœ‰æ ·æœ¬çš„yå‘é‡è¿›è¡Œä¸€æ¬¡è¿ç®—

on-lineå°±æ˜¯ä¸€æ¬¡è¿ç®—ä¸€ä¸ªsampleçš„xå‘é‡å’Œä¸€ä¸ªsampleçš„yå‘é‡

<img src="https://i.loli.net/2021/03/15/xtPinTlIeDbKzYL.png" alt="image.png" style="zoom:25%;" />

# 6.SGDMæ–¹æ³•

**å¸¦æœ‰momentumåŠ¨é‡çš„SGDéšæœºæ¢¯åº¦ä¸‹é™æ³•**

åŸç†ï¼š

å°±æ˜¯åœ¨SGDçš„åŸºç¡€ä¸Šï¼ŒåŠ ä¸Šå‰å‡ æ¬¡iterè¿­ä»£çš„ç´¯åŠ å’Œï¼Œç›®çš„æ˜¯é˜²æ­¢æŸä¸€æ¬¡çš„$\bigtriangledown L(\theta ^{0})$å¾ˆå°ï¼ˆæ¥è¿‘0ï¼‰ï¼Œåˆ™æƒé‡å‚æ•°wä¾¿å¡åœ¨æŸä¸€å¤„ï¼Œä¸ä¼šè¿›è¡Œæ›´æ–°äº†ã€‚

<img src="https://i.loli.net/2021/03/16/a7Ly51lcDzoZkXm.jpg" alt="æˆªå±2021-03-16 ä¸Šåˆ9.20.57.jpeg" style="zoom: 25%;" />
$$
v^0=0
\\
v^1=-\eta \triangledown L(\theta ^0)
\\
v^2=-\eta (\  \triangledown  L(\theta ^1)+\lambda \triangledown  L(\theta ^0))
\\
v^3=-\eta (\  \triangledown  L(\theta ^2)+\lambda \triangledown  L(\theta ^1)+\lambda^2 \triangledown  L(\theta ^0))
$$

å…¶ä¸­$\lambda$å†³å®šäº†æƒ¯æ€§å¯¹ä¸‹ä¸€æ¬¡æ–¹å‘æ›´æ–°çš„æƒé‡ï¼Œ



å›¾è§£ï¼š

åœ¨èµ°åˆ°ç¬¬ä¸‰æ­¥çš„æ—¶å€™ï¼ŒåŸæ¥çš„SGDæ–¹æ³•ä¼šè®¡ç®—æ¢¯åº¦ä¸º0ï¼Œåˆ™æƒé‡å‚æ•°wä¾¿ä¸è¿›è¡Œæ›´æ–°ï¼Œä¼šå¡åœ¨å±€éƒ¨æœ€å°ç‚¹çš„ä½ç½®ï¼Œä½†æ˜¯åŠ äº†MomentumåŠ¨é‡çš„è¯ï¼Œå®ƒè¿˜ä¼šç»§ç»­å¾€å‰æ›´æ–°æƒé‡å‚æ•°wï¼Œ

è¡¥å……ï¼šsaddle pointéç‚¹ï¼Œå°±æ˜¯å€’æ•°ä¸º0çš„ç‚¹ï¼Œä½†æ˜¯åˆä¸æ˜¯minimum

<img src="https://i.loli.net/2021/03/16/ADCFVmOcdKraEj8.png" alt="æˆªå±2021-03-16 ä¸Šåˆ10.30.38.png" style="zoom:25%;" />

# 7. RMSProp

åŸç†ï¼š

å°±æ˜¯æŠŠAdagradåˆ†æ¯éƒ¨åˆ†æ›¿æ¢ä¸ºä¸‹å›¾æ‰€ç¤ºï¼Œä¸ºäº†é˜²æ­¢å¦‚æœåˆšå¼€å§‹æ¢¯åº¦æ¯”è¾ƒå¤§çš„è¯Adagradæ–¹æ³•ä¼šæƒé‡å‚æ•°æ›´æ–°çš„è¿‡æ…¢ã€‚å› ä¸ºAdagradçš„åˆ†æ¯æ˜¯ä¸€ç›´ç´¯åŠ çš„ï¼Œä¸ä¼šå‡å°‘ï¼Œè€ŒRMSPropä¼šå‡å°‘åˆ†æ¯ã€‚ä»–çš„åˆ†æ¯æ˜¯å€Ÿé‰´äº†SGDMæ–¹æ³•çš„momentumåŠ¨é‡

<img src="https://i.loli.net/2021/03/16/QbFMvxDWwKymTuj.png" alt="image.png" style="zoom:25%;" />



# 8. Adam

adaptive moment estimation

Adam=SGDM+RMSProp

<img src="https://i.loli.net/2021/03/16/R1pH8GoQh7r3Isg.png" alt="image.png" style="zoom:100%;" />

# 9. åº”ç”¨

### computer versionçš„ç®—æ³•ï¼š

- YOLOç”¨SGDMè®­ç»ƒå‡ºæ¥çš„
- Mask R-CNNç”¨SGDMè®­ç»ƒå‡ºæ¥çš„
- ResNet ä¹Ÿæ˜¯ç”¨SGDMè®­ç»ƒå‡ºæ¥çš„

### ADAMè®­ç»ƒå‡ºæ¥çš„ä¸œè¥¿

- BERTç”¨æ¥åšæ–‡æ„ç†è§£ï¼ŒåšQAï¼Œç”Ÿæˆæ–‡ç« çš„model
- Transformer ç”¨æ¥åšç¿»è¯‘çš„model
- Tacotronç”¨neural networkåšè¯­éŸ³ç”Ÿæˆçš„model
- Big-GANåšç”Ÿæˆå½±åƒçš„model
- MEMOæ¼”ç®—æ³•ï¼Œè¿™ä¸ªmodelå¯ä»¥å¾ˆå®¹æ˜“å­¦ä¼šæ–°çš„task

### ADAMå’ŒSGDM

- Adamä¼˜ç‚¹ï¼š
  - å¿«é€Ÿçš„traningï¼Œå› ä¸ºAdamæ˜¯åœ¨å¹³ç¼“çš„åœ°æ–¹èµ°å¤§æ­¥ï¼Œsharpçš„åœ°æ–¹èµ°å°æ­¥ï¼Œæ‰€ä»¥æ¯”è¾ƒå¿«
- ç¼ºç‚¹ï¼š
  - å¤§çš„æ³›åŒ–å·®è·ï¼Œä¸ç¨³å®š
- SGDMä¼˜ç‚¹ï¼š
  - ç¨³å®šï¼Œå°çš„æ³›åŒ–å·®è·ï¼Œbetter convergenceç†Ÿç»ƒåˆ°å°çš„å€¼
- ç¼ºç‚¹ï¼š
  - æ…¢

æ³›åŒ–å·®è·generalization gapï¼šå°±æ˜¯æŒ‡è®­ç»ƒçš„å‚æ•°å’Œå®é™…æµ‹è¯•çš„å‚æ•°æœ‰ä¸ªoffsetï¼Œå°±æ˜¯ä¸‹å›¾ä¸­çš„trainingå’Œtestingä¸¤ä¸ªæœ€ä½ç‚¹çš„å·®è·ï¼Œå¯¹äºæ¯”è¾ƒå¹³ç¼“çš„loss functionæ¥è¯´gapä¸æ˜¯å¾ˆå¤§ï¼Œä½†æ˜¯å¯¹æ¯”è¾ƒsharpçš„Loss functionæ¥è¯´gapå½±å“å°±æ¯”è¾ƒå¤§

<img src="https://i.loli.net/2021/03/16/F3TNzA85pHIJRBa.png" alt="æˆªå±2021-03-16 ä¸‹åˆ2.45.48.png" style="zoom:25%;" />

# 10. ä¼˜åŒ–

## 10.1 ç»“åˆæ³•

å‰æœŸç”¨Adamï¼Œåé¢ç”¨SGDMï¼Œå«åšSWATS   [keskar,et al.,arXiv'17]

## 10.2 ä¼˜åŒ–Adam

[å˜çš„åˆç¨³åˆå¥½ ](https://www.youtube.com/watch?v=4pUmZ8hXlHM)





